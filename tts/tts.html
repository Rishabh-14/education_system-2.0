<!--
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenAI TTS</title>
</head>
<body>
    <textarea id="textInput" placeholder="Enter text here..."></textarea>
    <button onclick="submitText()">Submit</button>
    <audio id="audioPlayer" controls></audio>

    <script>
      async function submitText() {
        const textInput = document.getElementById('textInput').value;
        await fetchTTS(textInput);
      }

      async function fetchTTS(text) {
        try {
          const response = await fetch('http://localhost:3001/tts', {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
            },
            body: JSON.stringify({ text }),
          });

          if (!response.ok) {
            throw new Error('Network response was not ok');
          }

          const blob = await response.blob();
          const audioURL = URL.createObjectURL(blob);
          const audioPlayer = document.getElementById('audioPlayer');
          audioPlayer.src = audioURL;
          audioPlayer.play();
        } catch (error) {
          console.error('Fetch error:', error);
        }
      }
    </script>
</body>
</html>
-->

<!-- Working openai interruption
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Streaming Audio</title>
</head>
<body>
    <button onclick="startRecognition()">Start Recognition</button>
    <div id="text">Speech recognition is loading...</div>
    <audio id="audioPlayer" controls></audio>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/annyang/2.6.1/annyang.min.js"></script>
    <script>
        function startRecognition() {
            if (annyang) {
                initializeAnnyang();
                document.getElementById("text").textContent = "Speech recognition is enabled. Please speak.";
            } else {
                document.getElementById("text").textContent = "Speech Recognition is not supported";
            }
        }

        function initializeAnnyang() {
            var commands = {
                "*text": async function (transcript) {
                    document.getElementById("text").textContent = "Processing your input...";
                    try {
                        const response = await fetch("http://localhost:3001/audio", {
                            method: "POST",
                            headers: {
                                "Content-Type": "application/json",
                            },
                            body: JSON.stringify({ prompt: transcript }),
                        });

                        if (!response.ok) {
                            throw new Error('Network response was not ok');
                        }

                        const audioPlayer = document.getElementById("audioPlayer");
                        const mediaSource = new MediaSource();
                        audioPlayer.src = URL.createObjectURL(mediaSource);

                        mediaSource.addEventListener('sourceopen', () => {
                            const sourceBuffer = mediaSource.addSourceBuffer('audio/mpeg');
                            const reader = response.body.getReader();
                            const chunkQueue = [];

                            const processQueue = async () => {
                                if (chunkQueue.length > 0 && !sourceBuffer.updating) {
                                    sourceBuffer.appendBuffer(chunkQueue.shift());
                                }
                            };

                            sourceBuffer.addEventListener('updateend', processQueue);

                            const read = async () => {
                                try {
                                    const { done, value } = await reader.read();
                                    if (done) {
                                        mediaSource.endOfStream();
                                        console.log("Stream ended");
                                        return;
                                    }
                                    chunkQueue.push(value);
                                    processQueue();
                                    read();
                                } catch (error) {
                                    console.error('Error reading stream:', error);
                                }
                            };

                            read();
                        });

                        audioPlayer.play().catch(error => {
                            console.error('Error playing audio:', error);
                        });
                    } catch (error) {
                        document.getElementById("text").textContent = "Error processing your request";
                        console.error("Fetch error:", error);
                    }
                }
            };

            annyang.addCommands(commands);
            annyang.start({ autoRestart: true, continuous: true });
            annyang.debug();
        }
    </script>
</body>
</html>

-->
<!-- Audio + video working
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenCV Face Detection and Audio</title>
    <script async src="https://docs.opencv.org/master/opencv.js" type="text/javascript"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/annyang/2.6.1/annyang.min.js"></script>
</head>
<body>
    <video id="video" width="720" height="560" autoplay muted></video>
    <img id="output" width="720" height="560" alt="Face Detection Output">
    <div id="text">Speech recognition is enabled. Please speak.</div>
    <audio id="audioPlayer" controls></audio>

    <script>
        const video = document.getElementById('video');
        const output = document.getElementById('output');
        const textElement = document.getElementById('text');
        const audioPlayer = document.getElementById('audioPlayer');

        async function startVideo() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });
                video.srcObject = stream;
                console.log("Video and audio permissions granted.");
            } catch (error) {
                console.error("Error accessing media devices.", error);
            }
        }

        async function detectFace() {
            const canvas = document.createElement('canvas');
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            const context = canvas.getContext('2d');
            context.drawImage(video, 0, 0, canvas.width, canvas.height);
            const imageData = canvas.toDataURL('image/jpeg');

            const response = await fetch('http://localhost:3002/detect-face', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ image: imageData.split(',')[1] })
            });

            const result = await response.json();
            if (result.image) {
                output.src = `data:image/jpeg;base64,${result.image}`;
            } else {
                console.error('Error:', result.error);
            }
        }

        video.addEventListener('play', () => {
            setInterval(detectFace, 1000); // Check every second
        });

        startVideo();

        if (annyang) {
            const commands = {
                "*text": async function (transcript) {
                    console.log(`Recognized speech: ${transcript}`);
                    textElement.textContent = "Processing your input...";
                    try {
                        const response = await fetch("http://localhost:3001/audio", {
                            method: "POST",
                            headers: {
                                "Content-Type": "application/json",
                            },
                            body: JSON.stringify({ prompt: transcript }),
                        });

                        if (!response.ok) {
                            throw new Error('Network response was not ok');
                        }

                        console.log("Audio API called successfully.");

                        const reader = response.body.getReader();
                        const stream = new ReadableStream({
                            start(controller) {
                                function push() {
                                    reader.read().then(({ done, value }) => {
                                        if (done) {
                                            controller.close();
                                            return;
                                        }
                                        controller.enqueue(value);
                                        push();
                                    });
                                }

                                push();
                            }
                        });

                        const audioURL = URL.createObjectURL(new Blob([await new Response(stream).arrayBuffer()], { type: 'audio/ogg' }));
                        audioPlayer.src = audioURL;
                        audioPlayer.play();
                    } catch (error) {
                        textElement.textContent = "Error processing your request";
                        console.error("Fetch error:", error);
                    }
                }
            };

            annyang.addCommands(commands);
            annyang.start({ autoRestart: true, continuous: true });
            annyang.debug();
        } else {
            textElement.textContent = "Speech Recognition is not supported";
        }
    </script>
</body>
</html>
-->
<!-- Partially working
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenCV Face Detection and Audio</title>
    <script async src="https://docs.opencv.org/master/opencv.js" type="text/javascript"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/annyang/2.6.1/annyang.min.js"></script>
</head>
<body>
    <video id="video" width="720" height="560" autoplay muted></video>
    <img id="output" width="720" height="560" alt="Face Detection Output">
    <div id="text">Speech recognition is enabled. Please speak.</div>
    <audio id="audioPlayer" controls></audio>

    <script>
        const video = document.getElementById('video');
        const output = document.getElementById('output');
        const textElement = document.getElementById('text');
        const audioPlayer = document.getElementById('audioPlayer');
        let faceDetected = true;

        async function startVideo() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });
                video.srcObject = stream;
                console.log("Video and audio permissions granted.");
            } catch (error) {
                console.error("Error accessing media devices.", error);
            }
        }

        async function detectFace() {
            const canvas = document.createElement('canvas');
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            const context = canvas.getContext('2d');
            context.drawImage(video, 0, 0, canvas.width, canvas.height);
            const imageData = canvas.toDataURL('image/jpeg');

            const response = await fetch('http://localhost:3002/detect-face', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ image: imageData.split(',')[1] })
            });

            const result = await response.json();
            if (result.image) {
                output.src = `data:image/jpeg;base64,${result.image}`;
                if (!result.faceDetected && faceDetected) {
                    faceDetected = false;
                    console.log("Face not detected. Calling ChatGPT.");
                    await callChatGPT("tell a joke");
                } else if (result.faceDetected) {
                    faceDetected = true;
                }
            } else {
                console.error('Error:', result.error);
            }
        }

        async function callChatGPT(prompt) {
            try {
                const response = await fetch("http://localhost:3001/audio", {
                    method: "POST",
                    headers: {
                        "Content-Type": "application/json",
                    },
                    body: JSON.stringify({ prompt: prompt }),
                });

                if (!response.ok) {
                    throw new Error('Network response was not ok');
                }

                const reader = response.body.getReader();
                const stream = new ReadableStream({
                    start(controller) {
                        function push() {
                            reader.read().then(({ done, value }) => {
                                if (done) {
                                    controller.close();
                                    return;
                                }
                                controller.enqueue(value);
                                push();
                            });
                        }
                        push();
                    }
                });

                const audioURL = URL.createObjectURL(new Blob([await new Response(stream).arrayBuffer()], { type: 'audio/ogg' }));
                audioPlayer.src = audioURL;
                audioPlayer.play();
            } catch (error) {
                textElement.textContent = "Error processing your request";
                console.error("Fetch error:", error);
            }
        }

        video.addEventListener('play', () => {
            setInterval(detectFace, 1000); // Check every second
        });

        startVideo();

        if (annyang) {
            const commands = {
                "*text": async function (transcript) {
                    console.log(`Recognized speech: ${transcript}`);
                    textElement.textContent = "Processing your input...";
                    try {
                        const response = await fetch("http://localhost:3001/audio", {
                            method: "POST",
                            headers: {
                                "Content-Type": "application/json",
                            },
                            body: JSON.stringify({ prompt: transcript }),
                        });

                        if (!response.ok) {
                            throw new Error('Network response was not ok');
                        }

                        console.log("Audio API called successfully.");

                        const reader = response.body.getReader();
                        const stream = new ReadableStream({
                            start(controller) {
                                function push() {
                                    reader.read().then(({ done, value }) => {
                                        if (done) {
                                            controller.close();
                                            return;
                                        }
                                        controller.enqueue(value);
                                        push();
                                    });
                                }

                                push();
                            }
                        });

                        const audioURL = URL.createObjectURL(new Blob([await new Response(stream).arrayBuffer()], { type: 'audio/ogg' }));
                        audioPlayer.src = audioURL;
                        audioPlayer.play();
                    } catch (error) {
                        textElement.textContent = "Error processing your request";
                        console.error("Fetch error:", error);
                    }
                }
            };

            annyang.addCommands(commands);
            annyang.start({ autoRestart: true, continuous: true });
            annyang.debug();
        } else {
            textElement.textContent = "Speech Recognition is not supported";
        }
    </script>
</body>
</html>
-->

<!--
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenCV Face Detection and Audio</title>
    <script async src="https://docs.opencv.org/master/opencv.js" type="text/javascript"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/annyang/2.6.1/annyang.min.js"></script>
</head>
<body>
    <video id="video" width="720" height="560" autoplay muted></video>
    <img id="output" width="720" height="560" alt="Face Detection Output">
    <div id="label">Looking at the screen</div>
    <div id="text">Speech recognition is enabled. Please speak.</div>
    <audio id="audioPlayer" controls></audio>

    <script>
        const video = document.getElementById('video');
        const output = document.getElementById('output');
        const labelElement = document.getElementById('label');
        const textElement = document.getElementById('text');
        const audioPlayer = document.getElementById('audioPlayer');
        let faceDetected = true;
        let faceDetectionInitialized = false;

        async function startVideo() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });
                video.srcObject = stream;
                console.log("Video and audio permissions granted.");
            } catch (error) {
                console.error("Error accessing media devices.", error);
            }
        }

        async function detectFace() {
            const canvas = document.createElement('canvas');
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            const context = canvas.getContext('2d');
            context.drawImage(video, 0, 0, canvas.width, canvas.height);
            const imageData = canvas.toDataURL('image/jpeg');

            const response = await fetch('http://localhost:3002/detect-face', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ image: imageData.split(',')[1] })
            });

            const result = await response.json();
            if (result.image) {
                output.src = `data:image/jpeg;base64,${result.image}`;
                labelElement.textContent = result.label;
                if (!result.faceDetected && faceDetected && faceDetectionInitialized) {
                    faceDetected = false;
                    console.log("Face not detected. Calling ChatGPT.");
                    await callChatGPT("tell me a joke");
                } else if (result.faceDetected) {
                    faceDetected = true;
                }
                faceDetectionInitialized = true;
            } else {
                console.error('Error:', result.error);
            }
        }

        async function callChatGPT(prompt) {
            try {
                const response = await fetch("http://localhost:3001/audio", {
                    method: "POST",
                    headers: {
                        "Content-Type": "application/json",
                    },
                    body: JSON.stringify({ prompt: prompt }),
                });

                if (!response.ok) {
                    throw new Error('Network response was not ok');
                }

                const reader = response.body.getReader();
                const stream = new ReadableStream({
                    start(controller) {
                        function push() {
                            reader.read().then(({ done, value }) => {
                                if (done) {
                                    controller.close();
                                    return;
                                }
                                controller.enqueue(value);
                                push();
                            });
                        }
                        push();
                    }
                });

                const audioURL = URL.createObjectURL(new Blob([await new Response(stream).arrayBuffer()], { type: 'audio/ogg' }));
                audioPlayer.src = audioURL;
                audioPlayer.play();
            } catch (error) {
                textElement.textContent = "Error processing your request";
                console.error("Fetch error:", error);
            }
        }

        video.addEventListener('play', () => {
            setInterval(detectFace, 1000); // Check every second
        });

        startVideo();

        if (annyang) {
            const commands = {
                "*text": async function (transcript) {
                    console.log(`Recognized speech: ${transcript}`);
                    textElement.textContent = "Processing your input...";
                    try {
                        const response = await fetch("http://localhost:3001/audio", {
                            method: "POST",
                            headers: {
                                "Content-Type": "application/json",
                            },
                            body: JSON.stringify({ prompt: transcript }),
                        });

                        if (!response.ok) {
                            throw new Error('Network response was not ok');
                        }

                        console.log("Audio API called successfully.");

                        const reader = response.body.getReader();
                        const stream = new ReadableStream({
                            start(controller) {
                                function push() {
                                    reader.read().then(({ done, value }) => {
                                        if (done) {
                                            controller.close();
                                            return;
                                        }
                                        controller.enqueue(value);
                                        push();
                                    });
                                }

                                push();
                            }
                        });

                        const audioURL = URL.createObjectURL(new Blob([await new Response(stream).arrayBuffer()], { type: 'audio/ogg' }));
                        audioPlayer.src = audioURL;
                        audioPlayer.play();
                    } catch (error) {
                        textElement.textContent = "Error processing your request";
                        console.error("Fetch error:", error);
                    }
                }
            };

            annyang.addCommands(commands);
            annyang.start({ autoRestart: true, continuous: true });
            annyang.debug();
        } else {
            textElement.textContent = "Speech Recognition is not supported";
        }
    </script>
</body>
</html>
-->
<!-- Working distracted look away
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenCV Face Detection and Audio</title>
    <script async src="https://docs.opencv.org/master/opencv.js" type="text/javascript"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/annyang/2.6.1/annyang.min.js"></script>
    <script src="script.js" defer></script>
</head>
<body>
    <video id="video" width="720" height="560" autoplay muted></video>
    <img id="output" width="720" height="560" alt="Face Detection Output">
    <div id="label">Looking at the screen</div>
    <div id="text">Speech recognition is enabled. Please speak.</div>
    <audio id="audioPlayer" controls></audio>
</body>
</html>
-->

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenCV Face Detection and Audio</title>
    <script async src="https://docs.opencv.org/master/opencv.js" type="text/javascript"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/annyang/2.6.1/annyang.min.js"></script>
    <script src="script.js" defer></script>
</head>
<body>
    <video id="video" width="720" height="560" autoplay muted></video>
    <img id="output" width="720" height="560" alt="Face Detection Output">
    <div id="label">Looking at the screen</div>
    <div id="text">Speech recognition is enabled. Please speak.</div>
    <audio id="audioPlayer1" controls></audio>
    <audio id="audioPlayer2" controls></audio>
    <audio id="audioPlayer3" controls></audio>
</body>
</html>
